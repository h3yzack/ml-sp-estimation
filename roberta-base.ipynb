{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa model training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "case = \"Case_N\"\n",
    "code = f\"20x1_{case}\"\n",
    "dataset_splitted_path = f\"datasets/{case}\"\n",
    "output_dir = f\"./models/{case}/{code}/roberta\"\n",
    "\n",
    "batch_1 = ['APSTUD', 'BAM', 'CLOV', 'DM']\n",
    "batch_2 = ['DURACLOUD', 'JRESERVER', 'MDL', 'MESOS']\n",
    "batch_3 = ['MULE', 'MULESTUDIO', 'TIMOB', 'USERGRID']\n",
    "batch_4 = ['TISTUD', 'XD']\n",
    "\n",
    "# to run which batch?\n",
    "dataset_names = batch_1 + batch_2 + batch_3 + batch_4\n",
    "\n",
    "BASE_MODEL = \"roberta-base\"\n",
    "LEARNING_RATE = 1e-4     \n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    \n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    mdae = median_absolute_error(labels, logits)\n",
    "    return {\"mae\": mae, \"mdae\": mdae}\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    encoded = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    dataset = MakeTorchData(encoded, examples['storypoint'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class MakeTorchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        item[\"labels\"] = float(item[\"labels\"])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0][:, 0]\n",
    "        loss = torch.nn.functional.l1_loss(logits, labels)  # Use l1_loss for MAE\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "def rename_checkpoint_folder(output_dir):\n",
    "    dirs = os.listdir(output_dir)\n",
    "    # Loop over the directories\n",
    "    for dir in dirs:\n",
    "        # Check if the directory is a checkpoint directory\n",
    "        if dir.startswith(\"checkpoint-\"):\n",
    "            # Specify the current name and the new name of the checkpoint directory\n",
    "            current_name = os.path.join(output_dir, dir)\n",
    "            new_name = os.path.join(output_dir, \"model\" )\n",
    "\n",
    "            # Rename the checkpoint directory\n",
    "            shutil.move(current_name, new_name)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the cleaned data\n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    print(f\"start processing - {dataset_name}...\")\n",
    "\n",
    "    # load the json data\n",
    "    raw_train_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/train.json')\n",
    "    raw_val_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/val.json')\n",
    "    raw_test_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/test.json')\n",
    "\n",
    "    ds = {\"train\": preprocess_function(raw_train_data), \"validation\": preprocess_function(raw_val_data), \"test\": preprocess_function(raw_test_data)}\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n",
    "\n",
    "    file_dir = f\"{output_dir}/{dataset_name}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= file_dir,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        load_best_model_at_end=True,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        compute_metrics=compute_metrics_for_regression,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    # Evaluate the fine-tuned model\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(metrics)\n",
    "\n",
    "    metrics_json = json.dumps(metrics, indent=2)\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = f\"{file_dir}/{dataset_name}.json\"\n",
    "\n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(metrics_json)\n",
    "        \n",
    "    rename_checkpoint_folder(file_dir)\n",
    "\n",
    "    # if 1 == 1:\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_sp_env_3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
