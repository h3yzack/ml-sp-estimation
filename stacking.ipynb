{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 600\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_LENGTH = 256\n",
    "BASE_MODEL = \"roberta-base\"\n",
    "vocab = 10000\n",
    "\n",
    "case = \"Case_N\"\n",
    "code = f\"20x1_{case}\"\n",
    "dataset_splitted_path = f\"datasets/{case}\"\n",
    "models_path = f\"models/{case}/{code}\"\n",
    "\n",
    "\n",
    "batch_1 = ['APSTUD', 'BAM', 'CLOV', 'DM']\n",
    "batch_2 = ['DURACLOUD', 'JRESERVER', 'MDL', 'MESOS']\n",
    "batch_3 = ['MULE', 'MULESTUDIO', 'TIMOB']\n",
    "batch_4 = ['TISTUD', 'USERGRID', 'XD']\n",
    "\n",
    "# combine all datasets\n",
    "dataset_names = batch_1 + batch_2 + batch_3 + batch_4\n",
    "\n",
    "project_names = ['AS', 'BB', 'CV', 'DM', 'DC', 'JS', 'MD', 'ME', 'MU', 'MS', 'AP', 'TS', 'UG', 'XD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "import torch\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from keras.src.callbacks import EarlyStopping\n",
    "\n",
    "class MakeTorchData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        item[\"labels\"] = float(item[\"labels\"])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class RobertaRegressorWrapper:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.trainer = None\n",
    "        self.built = True\n",
    "        self.initFit()\n",
    "\n",
    "    def initFit(self):\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'{self.model_path}/ensemble',\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_eval_metrics,  # Add your own metrics function here if needed\n",
    "        )\n",
    "\n",
    "        # self.trainer.train()\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.trainer.eval_dataset = X\n",
    "        predictions = self.trainer.predict(X).predictions\n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"model_path\": self.model_path}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return mean_absolute_error(y, predictions)\n",
    "\n",
    "\n",
    "class BiLSTMRegressorWrapper:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model = load_model(model_path)\n",
    "        self.built = True\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "        # self.model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "        # self.model.fit(X, y, **kwargs)\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X).flatten()\n",
    "    \n",
    "def get_max_len(data, tokernizer):\n",
    "    tokernizer.fit_on_texts(data)\n",
    "    sequence_combined = tokernizer.texts_to_sequences(data)\n",
    "    max_len = max([len(x) for x in sequence_combined])\n",
    "    return max_len\n",
    "\n",
    "def compute_eval_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    mdae = median_absolute_error(labels, logits)\n",
    "    return {\"mae\": mae, \"mdae\": mdae}\n",
    "\n",
    "def preprocess_function(examples, tokernizer):\n",
    "    encoded = tokernizer(examples['text'], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    dataset = MakeTorchData(encoded, examples['storypoint'])\n",
    "    return dataset\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    raw_train_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/train.json')\n",
    "    raw_val_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/val.json')\n",
    "    raw_test_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/test.json')\n",
    "\n",
    "    return raw_train_data, raw_val_data, raw_test_data\n",
    "\n",
    "def load_dataset_type(dataset_name, dataset_type):\n",
    "    raw_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/{dataset_type}.json')\n",
    "    return raw_data\n",
    "\n",
    "def load_models(dataset_name):\n",
    "    bilstm_model_path = f'{models_path}/bilstm/{dataset_name}/{dataset_name}.keras'\n",
    "    roberta_model_path = f'{models_path}/roberta/{dataset_name}/model'\n",
    "\n",
    "    bilstm_wrapper = BiLSTMRegressorWrapper(bilstm_model_path)\n",
    "    roberta_wrapper = RobertaRegressorWrapper(roberta_model_path)\n",
    "\n",
    "    return bilstm_wrapper, roberta_wrapper\n",
    "\n",
    "def get_padding_sequence(data, tokenizer, max_len):\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "    padded_seq = pad_sequences(sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
    "    return padded_seq\n",
    "\n",
    "def get_meta_model():\n",
    "    meta_model = Sequential()\n",
    "    meta_model.add(Dense(10, input_dim=2, activation='relu'))  # Assuming you have 2 base models\n",
    "    meta_model.add(Dense(10, activation='relu'))  # Additional Dense layer\n",
    "    meta_model.add(Dense(1, activation='linear'))  # Final Dense layer with activation function\n",
    "\n",
    "    # Compile the meta-model\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    # optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "    meta_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    return meta_model\n",
    "\n",
    "def load_meta_model(dataset_name):\n",
    "    meta_model_path = f'{models_path}/meta-model/{dataset_name}/{dataset_name}.keras'\n",
    "    meta_model = load_model(meta_model_path)\n",
    "    return meta_model\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "bilstm_tokenizer = KerasTokenizer(num_words=vocab, oov_token=0)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"start processing - {dataset_name}...\")\n",
    "    os.makedirs(f'{models_path}/meta-model/{dataset_name}', exist_ok=True)\n",
    "\n",
    "    # load models\n",
    "    bilstm_model, roberta_model = load_models(dataset_name)\n",
    "\n",
    "    # load dataset\n",
    "    raw_train_data, raw_val_data, raw_test_data = load_dataset(dataset_name)\n",
    "\n",
    "    # calculate the max length of the sequences\n",
    "    max_len = get_max_len(pd.concat([pd.Series(raw_train_data['text']), pd.Series(raw_val_data['text'])]), bilstm_tokenizer)\n",
    "    print(f\"max_len: {max_len}\")\n",
    "\n",
    "    # convert raw_train_data to a DataFrame if it's not already\n",
    "    if not isinstance(raw_train_data, pd.DataFrame):\n",
    "        raw_train_data = pd.DataFrame(raw_train_data)\n",
    "\n",
    "    meta_model_inputs = []\n",
    "    meta_model_targets = []\n",
    "\n",
    "    for train_index, val_index in kf.split(raw_train_data):\n",
    "        # Split the data\n",
    "        raw_train_data_fold = raw_train_data.iloc[train_index]\n",
    "        raw_val_data_fold = raw_train_data.iloc[val_index]\n",
    "\n",
    "        # prepare for the RoBERTa model\n",
    "        roberta_train_data = preprocess_function(raw_train_data_fold.to_dict('list'), roberta_tokenizer)\n",
    "        roberta_val_data = preprocess_function(raw_val_data_fold.to_dict('list'), roberta_tokenizer)\n",
    "\n",
    "        # prepare for the BiLSTM model\n",
    "        train_data = pd.Series(raw_train_data_fold['text'])\n",
    "        train_padded_seq = get_padding_sequence(train_data, bilstm_tokenizer, max_len)\n",
    "        val_data = pd.Series(raw_val_data_fold['text'])\n",
    "        val_padded_seq = get_padding_sequence(val_data, bilstm_tokenizer, max_len)\n",
    "\n",
    "        # predict on validation fold\n",
    "        roberta_val_preds = roberta_model.predict(roberta_val_data)\n",
    "        bilstm_val_preds = bilstm_model.predict(val_padded_seq)\n",
    "\n",
    "        # Stack the predictions together\n",
    "        val_preds_meta = np.column_stack((roberta_val_preds, bilstm_val_preds))\n",
    "\n",
    "        # Add the predictions and targets to the lists\n",
    "        meta_model_inputs.append(val_preds_meta)\n",
    "        meta_model_targets.append(raw_val_data_fold['storypoint'])\n",
    "\n",
    "    # Concatenate all the predictions and targets\n",
    "    meta_model_inputs = np.concatenate(meta_model_inputs)\n",
    "    meta_model_targets = np.concatenate(meta_model_targets)\n",
    "\n",
    "    meta_model = get_meta_model()\n",
    "\n",
    "    # Define the checkpoint path and filename\n",
    "    checkpoint_filepath = f'{models_path}/meta-model/{dataset_name}/checkpoint'\n",
    "\n",
    "    # Create a ModelCheckpoint callback that saves the weights only of the best model observed as per the validation data\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "    \n",
    "    # Prepare the validation data for the base models\n",
    "    eval_label = pd.Series(raw_val_data['storypoint']).astype(float)\n",
    "    roberta_val_data = preprocess_function(raw_val_data, roberta_tokenizer)\n",
    "    val_padded_seq = get_padding_sequence(pd.Series(raw_val_data['text']), bilstm_tokenizer, max_len)\n",
    "\n",
    "    # Generate predictions from the base models on the validation set\n",
    "    roberta_val_preds = roberta_model.predict(roberta_val_data)\n",
    "    bilstm_val_preds = bilstm_model.predict(val_padded_seq)\n",
    "\n",
    "    # Stack the predictions together\n",
    "    val_preds_meta = np.column_stack((roberta_val_preds, bilstm_val_preds))\n",
    "\n",
    "    # Fit the model with the new callback\n",
    "    history = meta_model.fit(meta_model_inputs, meta_model_targets, validation_data=(val_preds_meta, eval_label), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=3), model_checkpoint_callback])\n",
    "\n",
    "    # Load the weights of the best model observed during training\n",
    "    meta_model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    # save the meta model\n",
    "    meta_model.save(f'{models_path}/meta-model/{dataset_name}/{dataset_name}.keras')\n",
    "\n",
    "    \n",
    "    val_loss, val_mae =  meta_model.evaluate(val_preds_meta, eval_label)\n",
    "    print(f\"val_loss: {val_loss}, val_mae: {val_mae}\")\n",
    "\n",
    "    if 1 == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "bilstm_tokenizer = KerasTokenizer(num_words=vocab, oov_token=0)\n",
    "\n",
    "results = []\n",
    "residuals_all = []\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"start processing - {dataset_name}...\")\n",
    "\n",
    "    # get project name\n",
    "    project_name = project_names[dataset_names.index(dataset_name)]\n",
    "\n",
    "    # load models\n",
    "    bilstm_model, roberta_model = load_models(dataset_name)\n",
    "    meta_model = load_meta_model(dataset_name)\n",
    "\n",
    "    # load dataset\n",
    "    raw_train_data, raw_val_data, raw_test_data = load_dataset(dataset_name)\n",
    "\n",
    "    # calculate the max length of the sequences\n",
    "    max_len = get_max_len(pd.concat([pd.Series(raw_train_data['text']), pd.Series(raw_val_data['text'])]), bilstm_tokenizer)\n",
    "    print(f\"max_len: {max_len}\")\n",
    "\n",
    "    # Prepare the validation data for the base models\n",
    "    eval_label = pd.Series(raw_val_data['storypoint']).astype(float)\n",
    "    roberta_val_data = preprocess_function(raw_val_data, roberta_tokenizer)\n",
    "    val_padded_seq = get_padding_sequence(pd.Series(raw_val_data['text']), bilstm_tokenizer, max_len)\n",
    "\n",
    "    # Generate predictions from the base models on the validation set\n",
    "    roberta_val_preds = roberta_model.predict(roberta_val_data)\n",
    "    bilstm_val_preds = bilstm_model.predict(val_padded_seq)\n",
    "\n",
    "    # calculate MAPE for each model\n",
    "    roberta_val_preds_flat = roberta_val_preds.flatten()\n",
    "    mape_roberta = np.mean(np.abs((eval_label - roberta_val_preds_flat) / eval_label))\n",
    "    mape_bilstm = np.mean(np.abs((eval_label - bilstm_val_preds) / eval_label))\n",
    "\n",
    "    print(f\"MAPE RoBERTa: {mape_roberta}\")\n",
    "    print(f\"MAPE BiLSTM: {mape_bilstm}\")\n",
    "\n",
    "    # Stack the predictions together\n",
    "    val_preds_meta = np.column_stack((roberta_val_preds, bilstm_val_preds))\n",
    "\n",
    "    stacking_pred_nn = meta_model.predict(val_preds_meta)\n",
    "\n",
    "    # save the predictions\n",
    "    np.save(f'{models_path}/meta-model/{dataset_name}/{dataset_name}_val_pred.npy', stacking_pred_nn)\n",
    "\n",
    "    # Calculate the MAE and MdAE\n",
    "    mae_nn = mean_absolute_error(eval_label, stacking_pred_nn)\n",
    "    mdae_nn = median_absolute_error(eval_label, stacking_pred_nn)\n",
    "    print(f\"NN Staking MdAE: {mdae_nn}\")\n",
    "    print(f\"NN Staking MAE: {mae_nn}\")\n",
    "    print(f\"Finish process {dataset_name}\")\n",
    "\n",
    "    val_storypoint = raw_val_data['storypoint']\n",
    "    # Standard Deviation of Residuals\n",
    "    std_dev = np.std(val_storypoint - stacking_pred_nn)\n",
    "\n",
    "    # Calculate the percentage error for each prediction\n",
    "    percentage_errors = (val_storypoint - stacking_pred_nn) / val_storypoint\n",
    "\n",
    "    # Calculate the mean percentage error\n",
    "    mpe = np.mean(percentage_errors)\n",
    "\n",
    "    # calculate the Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs(percentage_errors))\n",
    "    \n",
    "    result = {\n",
    "        'dataset': dataset_name,\n",
    "        'project_name': project_name,\n",
    "        'mae': mae_nn,\n",
    "        'mdae': mdae_nn,\n",
    "        'std_dev': std_dev,\n",
    "        'mpe': mpe,\n",
    "        'mape_stack': mape,\n",
    "        'mape_roberta': mape_roberta,\n",
    "        'mape_bilstm': mape_bilstm\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "    # if 1 == 1:\n",
    "    #     break\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{models_path}/meta-model/results.csv', index=False)\n",
    "   \n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_sp_env_3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
