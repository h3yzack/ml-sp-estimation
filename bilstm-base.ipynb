{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM Model training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"Case_M\"\n",
    "code = f\"20x1_{case}\"\n",
    "dataset_splitted_path = f\"datasets/{case}\"\n",
    "models_path = f\"models/{case}/{code}/bilstm\"\n",
    "embed_path = \"features/fastText\"\n",
    "fastText_path = \"../fastText/cc.en.300.bin\"\n",
    "\n",
    "# dataset_names = [\n",
    "#     'APSTUD', 'BAM', 'CLOV', 'DM', 'DURACLOUD', 'JRESERVER', 'MDL', 'MESOS', 'MULE', 'MULESTUDIO', 'TIMOB', 'TISTUD', 'USERGRID', 'XD'  \n",
    "# ]\n",
    "\n",
    "batch_1 = ['APSTUD', 'BAM', 'CLOV', 'DM']\n",
    "batch_2 = ['DURACLOUD', 'JRESERVER', 'MDL', 'MESOS']\n",
    "batch_3 = ['MULE', 'MULESTUDIO', 'TIMOB', 'USERGRID']\n",
    "batch_4 = ['TISTUD', 'XD']\n",
    "\n",
    "# combine all datasets\n",
    "dataset_names = batch_1 + batch_2 + batch_3 + batch_4\n",
    "\n",
    "vocab = 10001\n",
    "LEARNING_RATE = 1e-4   \n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "## Load the FastText model\n",
    "fasttext_model = load_facebook_vectors(fastText_path)\n",
    "\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    print(f\"start processing - {dataset_name}...\")\n",
    "\n",
    "    output_dir = f\"{embed_path}/{dataset_name}\"\n",
    "\n",
    "    # load the json data\n",
    "    raw_train_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/train.json')\n",
    "    raw_val_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/val.json')\n",
    "    raw_test_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/test.json')\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=vocab, oov_token='<OOV>')\n",
    "    combined = pd.concat([pd.Series(raw_train_data['text']), pd.Series(raw_val_data['text'])])\n",
    "    tokenizer.fit_on_texts(combined)\n",
    "\n",
    "    train = pd.Series(raw_train_data['text'])\n",
    "    val = pd.Series(raw_val_data['text'])\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train)\n",
    "    val_sequences = tokenizer.texts_to_sequences(val)\n",
    "\n",
    "    # Padding\n",
    "    max_len = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
    "    val_padded = pad_sequences(val_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
    "\n",
    "    # Create an embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab, 300))  # Assuming the FastText model dimensions are 300\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i <= 10000:  # Only consider the top num_words\n",
    "            if word in fasttext_model:\n",
    "                embedding_vector = fasttext_model[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # save the embedding matrix\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(f'{output_dir}/{dataset_name}-ft-embed.npy', embedding_matrix)\n",
    "\n",
    "print(\"done\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM model training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def get_model_new(embedding_matrix, max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim = vocab,\n",
    "        output_dim = 300, \n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=True))\n",
    "    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, recurrent_dropout=0.1)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    print(f\"start processing - {dataset_name}...\")\n",
    "\n",
    "    output_dir = f\"{models_path}/{dataset_name}\"\n",
    "\n",
    "    # load the json data\n",
    "    raw_train_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/train.json')\n",
    "    raw_val_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/val.json')\n",
    "    raw_test_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/test.json')\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=vocab, oov_token='<OOV>')\n",
    "    combined = pd.concat([pd.Series(raw_train_data['text']), pd.Series(raw_val_data['text'])])\n",
    "    tokenizer.fit_on_texts(combined)\n",
    "\n",
    "    train = pd.Series(raw_train_data['text'])\n",
    "    val = pd.Series(raw_val_data['text'])\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train)\n",
    "    val_sequences = tokenizer.texts_to_sequences(val)\n",
    "\n",
    "    # Padding\n",
    "    max_len = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
    "    val_padded = pad_sequences(val_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
    "\n",
    "    # Load embedding matrix\n",
    "    embedding_matrix = np.load(f'{embed_path}/{dataset_name}/{dataset_name}-ft-embed.npy')\n",
    "\n",
    "    model = get_model_new(embedding_matrix, max_len)\n",
    "\n",
    "    # optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "    # Generate the plot\n",
    "    # plot_model(model, to_file=f'{output_dir}/{dataset_name}_model_plot.png', show_shapes=True, show_layer_names=False, show_layer_activations=True)\n",
    "\n",
    "    # Fit the model\n",
    "    train_label = pd.Series(raw_train_data['storypoint'])\n",
    "    val_label = pd.Series(raw_val_data['storypoint'])\n",
    "\n",
    "    history = model.fit(train_padded, train_label, validation_data=(val_padded, val_label), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save(f'{output_dir}/{dataset_name}.keras')\n",
    "\n",
    "    val_loss, val_mae = model.evaluate(val_padded, val_label, verbose=1)\n",
    "    print(f\"val_loss: {val_loss}, val_mae: {val_mae}\")\n",
    "\n",
    "    # test\n",
    "    test = pd.Series(raw_test_data['text'])\n",
    "    test_sequences = tokenizer.texts_to_sequences(test)\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
    "\n",
    "    test_label = pd.Series(raw_test_data['storypoint'])\n",
    "    test_loss, test_mae = model.evaluate(test_padded, test_label, verbose=1)\n",
    "    print(f\"test_loss: {test_loss}, test_mae: {test_mae}\")\n",
    "\n",
    "    metrics = { \n",
    "        'type': 'bilstm',\n",
    "        'val_loss': val_loss, 'val_mae': val_mae, \n",
    "        'test_loss': test_loss, 'test_mae': test_mae, \n",
    "        'epoch': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE\n",
    "    }\n",
    "\n",
    "    metrics_json = json.dumps(metrics, indent=2)\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = f\"{output_dir}/{dataset_name}.json\"\n",
    "\n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(metrics_json)\n",
    "\n",
    "    # if 1 == 1:\n",
    "    #     break\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "\n",
    "# Assume vocab, dataset_names, dataset_splitted_path, models_path are already defined\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Processing test predictions for {dataset_name}...\")\n",
    "\n",
    "    output_dir = f\"{models_path}/{dataset_name}\"\n",
    "\n",
    "    # Load test data\n",
    "    raw_test_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/test.json')\n",
    "    test_texts = pd.Series(raw_test_data['text'])\n",
    "    test_labels = pd.Series(raw_test_data['storypoint'])\n",
    "\n",
    "    # Load tokenizer (fit on train+val as before)\n",
    "    raw_train_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/train.json')\n",
    "    raw_val_data = Dataset.from_json(f'{dataset_splitted_path}/{dataset_name}/val.json')\n",
    "    tokenizer = Tokenizer(num_words=vocab, oov_token='<OOV>')\n",
    "    combined = pd.concat([pd.Series(raw_train_data['text']), pd.Series(raw_val_data['text'])])\n",
    "    tokenizer.fit_on_texts(combined)\n",
    "\n",
    "    # Tokenize and pad test data\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "    # Get max_len as used in training\n",
    "    train_sequences = tokenizer.texts_to_sequences(pd.Series(raw_train_data['text']))\n",
    "    val_sequences = tokenizer.texts_to_sequences(pd.Series(raw_val_data['text']))\n",
    "    max_len = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0)\n",
    "\n",
    "    # Load model\n",
    "    model_path = f\"{output_dir}/{dataset_name}.keras\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Predict\n",
    "    test_predictions = model.predict(test_padded).flatten()\n",
    "\n",
    "    # Save predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'true_storypoint': test_labels,\n",
    "        'predicted_storypoint': test_predictions\n",
    "    })\n",
    "    predictions_df.to_csv(f\"{output_dir}/{dataset_name}_predictions.csv\", index=False)\n",
    "\n",
    "    print(f\"Saved predictions for {dataset_name}\")\n",
    "\n",
    "    # if 1 == 1:\n",
    "    #     break\n",
    "\n",
    "print(\"All predictions saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_sp_env_3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
