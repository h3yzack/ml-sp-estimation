{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zuhaimi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zuhaimi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zuhaimi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import html2text as h2t\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import zscore\n",
    "from datasets import Dataset\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "dataset_path = \"datasets/Choet_Dataset\"\n",
    "dataset_splitted_path_raw = \"datasets/Choet_Dataset_Raw\"\n",
    "dataset_splitted_path_N = \"datasets/Case_N\"\n",
    "dataset_splitted_path_M = \"datasets/Case_M\"\n",
    "\n",
    "choet_filenames = [\n",
    "    'APSTUD_deep-se.csv',\n",
    "    'BAM_deep-se.csv',\n",
    "    'CLOV_deep-se.csv',\n",
    "    'DM_deep-se.csv',\n",
    "    'DURACLOUD_deep-se.csv',\n",
    "    'JRESERVER_deep-se.csv',\n",
    "    'MDL_deep-se.csv',\n",
    "    'MESOS_deep-se.csv',\n",
    "    'MULE_deep-se.csv',\n",
    "    'MULESTUDIO_deep-se.csv',\n",
    "    'TIMOB_deep-se.csv',\n",
    "    'TISTUD_deep-se.csv',\n",
    "    'USERGRID_deep-se.csv',\n",
    "    'XD_deep-se.csv'\n",
    "]\n",
    "\n",
    "# dataset_names = [\n",
    "#     'APSTUD', 'BAM', 'CLOV', 'DM', 'DURACLOUD', 'JRESERVER', 'MDL', 'MESOS', 'MULE', 'MULESTUDIO', 'TIMOB', 'TISTUD', 'USERGRID', 'XD'  \n",
    "# ]\n",
    "\n",
    "batch_1 = ['APSTUD', 'BAM', 'CLOV', 'DM']\n",
    "batch_2 = ['DURACLOUD', 'JRESERVER', 'MDL', 'MESOS']\n",
    "batch_3 = ['MULE', 'MULESTUDIO', 'TIMOB', 'USERGRID']\n",
    "batch_4 = ['TISTUD', 'XD']\n",
    "\n",
    "dataset_names = batch_1 + batch_2\n",
    "\n",
    "project_names = ['AS', 'BB', 'CV', 'DM', 'DC', 'JS', 'MD', 'ME', 'MU', 'MS', 'AP', 'TS', 'UG', 'XD']\n",
    "\n",
    "def extract_markdown(text):\n",
    "    h = h2t.HTML2Text()\n",
    "    h.ignore_links = True\n",
    "    text = h.handle(text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join words back to text\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.fillna('')\n",
    "    text = text.apply(extract_markdown)\n",
    "    text = text.apply(remove_urls)\n",
    "    text = text.apply(preprocess_text)\n",
    "    return text\n",
    "\n",
    "def load_split_file(dataset_name, type='train'):\n",
    "    path = os.path.join(dataset_splitted_path_A, dataset_name, f'{type}.csv')\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.fillna(' ')\n",
    "\n",
    "    # generate new dataframe\n",
    "    d = {'text': (df['title'] + '. ' + df['description']).tolist(), 'storypoint': df['storypoint'], 'issuekey': df['issuekey']}\n",
    "    dfx = pd.DataFrame(data=d)\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['issuekey', 'text', 'storypoint'])\n",
    "    train_data = pd.concat([train_data, dfx], ignore_index=False)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def load_clean_data(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "\n",
    "    # clean title and description\n",
    "    dataset['title'] = clean_text(dataset['title'])\n",
    "    dataset['description'] = clean_text(dataset['description'])\n",
    "    dataset['text'] = pd.concat([dataset['title'], dataset['description']], axis=1).apply(lambda x: '. '.join(x), axis=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_data_ordered(data):\n",
    "    trainingSize = 60\n",
    "    validationSize = 20\n",
    "    testSize = 20\n",
    "\n",
    "    numData = len(data)\n",
    "    numTrain = (trainingSize * numData) / 100\n",
    "    numVal = (validationSize * numData) / 100\n",
    "    numTest = (testSize * numData) / 100\n",
    "\n",
    "    # round the numbers\n",
    "    numTrain = math.floor(numTrain)\n",
    "    numVal = math.floor(numVal)\n",
    "    numTest = math.floor(numTest)\n",
    "\n",
    "    # split the dataset into train, validation, and test sets\n",
    "    train_data = data.iloc[0:numTrain - 1]\n",
    "    val_data = data.iloc[numTrain - 1:numTrain + numVal - 1]\n",
    "    test_data = data.iloc[numTrain + numVal - 1:numData]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def split_data(data, labels, test_size=0.2, random_state=42):\n",
    "\n",
    "    train_data, temp_data, train_labels, temp_labels = train_test_split(data, labels, test_size=test_size, random_state=random_state)\n",
    "    val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, random_state=random_state)\n",
    "\n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels\n",
    "\n",
    "# def drop_outliers(df, field_name):\n",
    "#     iqr = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))\n",
    "#     df.drop(df[df[field_name] > (iqr + np.percentile(df[field_name], 75))].index, inplace=True)\n",
    "#     df.drop(df[df[field_name] < (np.percentile(df[field_name], 25) - iqr)].index, inplace=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "from scipy.stats import iqr\n",
    "\n",
    "def drop_outliers(df, field_name):\n",
    "    Q1 = df[field_name].quantile(0.25)\n",
    "    Q3 = df[field_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter out the outliers\n",
    "    df_out = df.loc[(df[field_name] > lower_bound) & (df[field_name] < upper_bound)]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def drop_outliers_modified_z_score(df, field_name, threshold=3.5):\n",
    "    median = df[field_name].median()\n",
    "    median_absolute_deviation = np.median(np.abs(df[field_name] - median))\n",
    "    \n",
    "    # Add a small constant to avoid division by zero\n",
    "    median_absolute_deviation = median_absolute_deviation if median_absolute_deviation != 0 else 1e-10\n",
    "\n",
    "    modified_z_scores = 0.6745 * (df[field_name] - median) / median_absolute_deviation\n",
    "\n",
    "    # Filter out the outliers\n",
    "    df_out = df.loc[np.abs(modified_z_scores) <= threshold]\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def cut_of90(labels):\n",
    "        # This method is copied from the original code of the paper \"A Deep Learning Model for Estimating Story Points by Choetkiertikul et al. (2019)\"\n",
    "        #         To investigate and compare the performance of the proposed model with state-of-the-art models\n",
    "        val_y = list(set(labels))\n",
    "        val_y.sort()\n",
    "        l_dict = dict()\n",
    "        for i, val in enumerate(val_y): \n",
    "            l_dict[int(val)] = i\n",
    "\n",
    "        count_y = [0] * len(val_y)\n",
    "    \n",
    "        for label in labels:\n",
    "            count_y[l_dict[int(label)]] += 1\n",
    "\n",
    "        n_samples = len(labels)\n",
    "        s, threshold = 0, 0\n",
    "        for i, c in enumerate(count_y):\n",
    "            s += c\n",
    "            if s * 10 >= n_samples * 9:\n",
    "                threshold = val_y[i]\n",
    "                break\n",
    "        for i, l in enumerate(labels):\n",
    "            labels[i] = min(threshold, l)\n",
    "\n",
    "        return labels.astype('float32')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case N: Preprocessing & remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validation/Test data for APSTUD: 491 - 165 - 168 (824)\n",
      "Train/Validation/Test data for BAM: 294 - 104 - 106 (504)\n",
      "Train/Validation/Test data for CLOV: 197 - 76 - 79 (352)\n",
      "Train/Validation/Test data for DM: 2480 - 933 - 935 (4348)\n",
      "Train/Validation/Test data for DURACLOUD: 378 - 133 - 135 (646)\n",
      "Train/Validation/Test data for JRESERVER: 204 - 70 - 72 (346)\n",
      "Train/Validation/Test data for MDL: 657 - 233 - 235 (1125)\n",
      "Train/Validation/Test data for MESOS: 978 - 336 - 337 (1651)\n",
      "Train/Validation/Test data for MULE: 525 - 177 - 180 (882)\n",
      "Train/Validation/Test data for MULESTUDIO: 431 - 146 - 148 (725)\n",
      "Train/Validation/Test data for TIMOB: 1238 - 450 - 452 (2140)\n",
      "Train/Validation/Test data for TISTUD: 1719 - 583 - 586 (2888)\n",
      "Train/Validation/Test data for USERGRID: 250 - 96 - 98 (444)\n",
      "Train/Validation/Test data for XD: 2011 - 705 - 707 (3423)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for filename in choet_filenames:\n",
    "    dataset = load_data(f'{dataset_path}/{filename}')\n",
    "\n",
    "    dataset_name = filename.split('_')[0]\n",
    "    project_name = project_names[choet_filenames.index(filename)]\n",
    "\n",
    "    train_data, val_data, test_data = split_data_ordered(dataset)\n",
    "    \n",
    "    # clean the data\n",
    "    train_data.loc[:, 'text'] = clean_text(train_data['text'])\n",
    "    val_data.loc[:, 'text'] = clean_text(val_data['text'])\n",
    "    test_data.loc[:, 'text'] = clean_text(test_data['text'])\n",
    "\n",
    "    # save the datasets\n",
    "    os.makedirs(f'{dataset_splitted_path_N}/{dataset_name}', exist_ok=True)\n",
    "\n",
    "    # remove outlier on train data\n",
    "    train_data = drop_outliers(train_data, 'storypoint')\n",
    "\n",
    "    record = {\n",
    "        'dataset': project_name,\n",
    "        'num_records': len(train_data),\n",
    "        'min': train_data['storypoint'].min(),\n",
    "        'max': train_data['storypoint'].max(),\n",
    "        'mean': train_data['storypoint'].mean(),\n",
    "        'median': train_data['storypoint'].median()\n",
    "    } \n",
    "    records.append(record)\n",
    "\n",
    "    # save the datasets to json\n",
    "    train_data.to_json(f'{dataset_splitted_path_N}/{dataset_name}/train.json', orient='records', lines=True)\n",
    "    val_data.to_json(f'{dataset_splitted_path_N}/{dataset_name}/val.json', orient='records', lines=True)\n",
    "    test_data.to_json(f'{dataset_splitted_path_N}/{dataset_name}/test.json', orient='records', lines=True)\n",
    "\n",
    "    # print length of each dataset\n",
    "    print(f\"Train/Validation/Test data for {dataset_name}: {len(train_data)} - {len(val_data)} - {len(test_data)} ({len(train_data) + len(val_data) + len(test_data)})\")\n",
    "    \n",
    "    # if 1 == 1:\n",
    "    #     break  \n",
    "# save the records to csv\n",
    "records_df = pd.DataFrame(records)\n",
    "records_df.to_csv(f'{dataset_splitted_path_N}/records.csv', index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case M: Preprocessing & Transform outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validation/Test data for APSTUD: 496 - 165 - 168 (829)\n",
      "Train/Validation/Test data for BAM: 311 - 104 - 106 (521)\n",
      "Train/Validation/Test data for CLOV: 229 - 76 - 79 (384)\n",
      "Train/Validation/Test data for DM: 2799 - 933 - 935 (4667)\n",
      "Train/Validation/Test data for DURACLOUD: 398 - 133 - 135 (666)\n",
      "Train/Validation/Test data for JRESERVER: 210 - 70 - 72 (352)\n",
      "Train/Validation/Test data for MDL: 698 - 233 - 235 (1166)\n",
      "Train/Validation/Test data for MESOS: 1007 - 336 - 337 (1680)\n",
      "Train/Validation/Test data for MULE: 532 - 177 - 180 (889)\n",
      "Train/Validation/Test data for MULESTUDIO: 438 - 146 - 148 (732)\n",
      "Train/Validation/Test data for TIMOB: 1349 - 450 - 452 (2251)\n",
      "Train/Validation/Test data for TISTUD: 1750 - 583 - 586 (2919)\n",
      "Train/Validation/Test data for USERGRID: 288 - 96 - 98 (482)\n",
      "Train/Validation/Test data for XD: 2114 - 705 - 707 (3526)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "records = []\n",
    "\n",
    "for filename in choet_filenames:\n",
    "    # print(f\"start processing - {filename}...\")\n",
    "    dataset = load_data(f'{dataset_path}/{filename}')\n",
    "\n",
    "    dataset_name = filename.split('_')[0]\n",
    "    project_name = project_names[choet_filenames.index(filename)]\n",
    "\n",
    "    # print(\"max value in dataset column 'storypoint': \", dataset['storypoint'].max())\n",
    "\n",
    "    # 90% transformation\n",
    "    cut_of90(dataset['storypoint'])\n",
    "\n",
    "    # print max and min values in column 'storypoint'\n",
    "    # print(\"max value in column 'storypoint': \", dataset['storypoint'].max())\n",
    "\n",
    "    train_data, val_data, test_data = split_data_ordered(dataset)\n",
    "    \n",
    "    # clean the train data\n",
    "    train_data.loc[:, 'text'] = clean_text(train_data['text'])\n",
    "    val_data.loc[:, 'text'] = clean_text(val_data['text'])\n",
    "    test_data.loc[:, 'text'] = clean_text(test_data['text'])\n",
    "\n",
    "    record = {\n",
    "        'dataset': project_name,\n",
    "        'num_records': len(train_data),\n",
    "        'min': train_data['storypoint'].min(),\n",
    "        'max': train_data['storypoint'].max(),\n",
    "        'mean': train_data['storypoint'].mean(),\n",
    "        'median': train_data['storypoint'].median()\n",
    "    } \n",
    "    records.append(record)\n",
    "\n",
    "    # save the datasets\n",
    "    os.makedirs(f'{dataset_splitted_path_M}/{dataset_name}', exist_ok=True)\n",
    "\n",
    "    # save the datasets to json\n",
    "    train_data.to_json(f'{dataset_splitted_path_M}/{dataset_name}/train.json', orient='records', lines=True)\n",
    "    val_data.to_json(f'{dataset_splitted_path_M}/{dataset_name}/val.json', orient='records', lines=True)\n",
    "    test_data.to_json(f'{dataset_splitted_path_M}/{dataset_name}/test.json', orient='records', lines=True)\n",
    "\n",
    "    # print length of each dataset\n",
    "    print(f\"Train/Validation/Test data for {dataset_name}: {len(train_data)} - {len(val_data)} - {len(test_data)} ({len(train_data) + len(val_data) + len(test_data)})\")\n",
    "    \n",
    "    # if 1 == 1:\n",
    "    #     break  \n",
    "     \n",
    "# save the records to csv\n",
    "records_df = pd.DataFrame(records)\n",
    "records_df.to_csv(f'{dataset_splitted_path_M}/records.csv', index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_sp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
